\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{Hall_weka:2010}
\citation{Frank+Asuncion:2010}
\citation{Frank+Asuncion:2010}
\citation{Mallah:2013}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction of Datasets}{1}{section.1}}
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{1}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Analysis of Wisconsin Dataset}{1}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Introduction}{1}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Data Analysis}{1}{subsection.3.2}}
\@LN@col{1}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of Algorithm Runtime\relax }}{2}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:avgtimed1title}{{1}{2}{Comparison of Algorithm Runtime\relax \relax }{table.caption.1}{}}
\newlabel{tb:avgtimed1}{{1}{2}{Comparison of Algorithm Runtime\relax \relax }{table.caption.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of Algorithm Performance\relax }}{2}{table.caption.2}}
\newlabel{tab:bestaccd1title}{{2}{2}{Comparison of Algorithm Performance\relax \relax }{table.caption.2}{}}
\newlabel{tb:bestaccd1}{{2}{2}{Comparison of Algorithm Performance\relax \relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Decision Trees}{2}{subsubsection.3.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces This graph demonstrates the little effect that the confidence factor had on the dataset.\relax }}{2}{figure.caption.3}}
\newlabel{fig:j48d1}{{1}{2}{This graph demonstrates the little effect that the confidence factor had on the dataset.\relax \relax }{figure.caption.3}{}}
\@LN@col{2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Neural Networks}{2}{subsubsection.3.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Here I analyze the training time in order to understand if it has any effect on the accuracy.\relax }}{2}{figure.caption.4}}
\newlabel{fig:nn3d1}{{2}{2}{Here I analyze the training time in order to understand if it has any effect on the accuracy.\relax \relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The training for neural networks with the benign and malignant true positive rates. \relax }}{2}{figure.caption.5}}
\newlabel{fig:nn1d1}{{3}{2}{The training for neural networks with the benign and malignant true positive rates. \relax \relax }{figure.caption.5}{}}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The testing results for neural networks, with benign and malignant true positive rates. \relax }}{3}{figure.caption.6}}
\newlabel{fig:nn2d1}{{4}{3}{The testing results for neural networks, with benign and malignant true positive rates. \relax \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Boosting}{3}{subsubsection.3.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces As expected, boosting returns 100\% as can be seen in the train accuracy section. The interval for the number of iterations starts at 10, and then goes to 100, and from there rises in intervals of 100. This is why the graphs indicate a huge increment at the leftmost part of the graph. We tested the algorithm until 2000 iterations. The malignant and benign true positive results are after cross validation (testing), since training performed perfectly at 100+ iterations.\relax }}{3}{figure.caption.7}}
\newlabel{fig:bb1d1}{{5}{3}{As expected, boosting returns 100\% as can be seen in the train accuracy section. The interval for the number of iterations starts at 10, and then goes to 100, and from there rises in intervals of 100. This is why the graphs indicate a huge increment at the leftmost part of the graph. We tested the algorithm until 2000 iterations. The malignant and benign true positive results are after cross validation (testing), since training performed perfectly at 100+ iterations.\relax \relax }{figure.caption.7}{}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The best performance for boosting on different classifiers was LMT.\relax }}{3}{figure.caption.8}}
\newlabel{fig:bb2d1}{{6}{3}{The best performance for boosting on different classifiers was LMT.\relax \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Support Vector Machines}{3}{subsubsection.3.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Here it can be observed that accuracy did not seem to have a correlation with complexity.\relax }}{3}{figure.caption.9}}
\newlabel{fig:svm1d1}{{7}{3}{Here it can be observed that accuracy did not seem to have a correlation with complexity.\relax \relax }{figure.caption.9}{}}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces This demonstrates the difference in performance for different kernels.\relax }}{4}{figure.caption.10}}
\newlabel{fig:svm2d1}{{8}{4}{This demonstrates the difference in performance for different kernels.\relax \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.5}K-Nearest Neighbors}{4}{subsubsection.3.2.5}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Training for knn demonstrates a loss in performance for malignant TP rates.\relax }}{4}{figure.caption.11}}
\newlabel{fig:knn2d1}{{9}{4}{Training for knn demonstrates a loss in performance for malignant TP rates.\relax \relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Testing performed on different k parameters through 10-fold cross-validation.\relax }}{4}{figure.caption.12}}
\newlabel{fig:knn1d1}{{10}{4}{Testing performed on different k parameters through 10-fold cross-validation.\relax \relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Performance Comparison}{4}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Analysis of Plant Species Dataset}{4}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Introduction}{4}{subsection.4.1}}
\@LN@col{1}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of Algorithm Runtime\relax }}{5}{table.caption.13}}
\newlabel{tab:avgtimed2title}{{3}{5}{Comparison of Algorithm Runtime\relax \relax }{table.caption.13}{}}
\newlabel{tb:avgtimed2}{{3}{5}{Comparison of Algorithm Runtime\relax \relax }{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Comparison of Algorithm Performance\relax }}{5}{table.caption.14}}
\newlabel{tab:bestaccd2title}{{4}{5}{Comparison of Algorithm Performance\relax \relax }{table.caption.14}{}}
\newlabel{tb:bestaccd2}{{4}{5}{Comparison of Algorithm Performance\relax \relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Data Analysis}{5}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Decision Trees}{5}{subsubsection.4.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Varying the confidence factor of the j48 algorithm.\relax }}{5}{figure.caption.15}}
\newlabel{fig:j481d2}{{11}{5}{Varying the confidence factor of the j48 algorithm.\relax \relax }{figure.caption.15}{}}
\@LN@col{2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Neural Networks}{5}{subsubsection.4.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The learning rate of neural networks did not have a significant effect after incrementing past a learning rate of 0.10.\relax }}{5}{figure.caption.16}}
\newlabel{fig:nn1d2}{{12}{5}{The learning rate of neural networks did not have a significant effect after incrementing past a learning rate of 0.10.\relax \relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces We graphed the clock time against its learning rate and there seemed to be no correlation.\relax }}{5}{figure.caption.17}}
\newlabel{fig:nn2d2}{{13}{5}{We graphed the clock time against its learning rate and there seemed to be no correlation.\relax \relax }{figure.caption.17}{}}
\@LN@col{1}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Boosting}{6}{subsubsection.4.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The incredibly poor accuracy of using decision stump in the boosting algorithm.\relax }}{6}{figure.caption.18}}
\newlabel{fig:boost1d2}{{14}{6}{The incredibly poor accuracy of using decision stump in the boosting algorithm.\relax \relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The accuracy of training and testing by using J48.\relax }}{6}{figure.caption.19}}
\newlabel{fig:boost2d2}{{15}{6}{The accuracy of training and testing by using J48.\relax \relax }{figure.caption.19}{}}
\@LN@col{2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}SVMs}{6}{subsubsection.4.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Accuracy after a complexity of 1.5 stalls at approximately 97.24\% for training, and 81.17\% for testing.\relax }}{6}{figure.caption.20}}
\newlabel{fig:svm1d2}{{16}{6}{Accuracy after a complexity of 1.5 stalls at approximately 97.24\% for training, and 81.17\% for testing.\relax \relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Accuracy stalls after about 140 seconds of performance.\relax }}{6}{figure.caption.21}}
\newlabel{fig:svm2d2}{{17}{6}{Accuracy stalls after about 140 seconds of performance.\relax \relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}K-Nearest Neighbors}{6}{subsubsection.4.2.5}}
\citation{Frank+Asuncion:2010}
\citation{Mallah:2013}
\bibstyle{acmsiggraph}
\bibdata{machine_learning_p1}
\bibcite{Mallah:2013}{\citename {Charles\nobreakspace  {}Mallah and Orwell }2013}
\bibcite{Frank+Asuncion:2010}{\citename {Frank and Asuncion }2010}
\bibcite{Hall_weka:2010}{\citename {Hall et\nobreakspace  {}al\unhbox \voidb@x \hbox {.} }2010}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces **The labels are flipped in this diagram. Blue represents the training data, and red represents the testing data. The figure demonstrates the drop in performance as we increment k.\relax }}{7}{figure.caption.22}}
\newlabel{fig:knn1d2}{{18}{7}{**The labels are flipped in this diagram. Blue represents the training data, and red represents the testing data. The figure demonstrates the drop in performance as we increment k.\relax \relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Performance Comparison}{7}{subsection.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results - General Comparison}{7}{section.5}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{7}{section.6}}
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{\numberline {7}Future Work}{7}{section.7}}
